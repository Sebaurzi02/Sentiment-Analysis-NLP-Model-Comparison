{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2294a504-8f65-4af1-8f17-69301bb2ced9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using LDA with PCA and Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ff0e29-166c-43f4-b61c-c9d2c0028b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk # Natural Language Processing\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Bag of Words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDF\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords  # List of common words usually ignored in text analysis\n",
    "#from nltk.stem.porter import PorterStemmer  # For stemming using the Porter algorithm\n",
    "from wordcloud import WordCloud, STOPWORDS  # To create word cloud visualizations from text\n",
    "from nltk.stem import WordNetLemmatizer  # For lemmatization\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # For word and sentence tokenization\n",
    "from bs4 import BeautifulSoup  # For HTML and XML parsing (web scraping)\n",
    "import spacy  # Fast and efficient NLP library\n",
    "import re, string, unicodedata  # For string manipulation and text normalization\n",
    "from nltk.tokenize.toktok import ToktokTokenizer  # More efficient tokenizer\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer  # Alternatives for stemming and lemmatization\n",
    "from textblob import TextBlob  # For text and sentiment analysis\n",
    "from textblob import Word  # For word processing\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # For model evaluation\n",
    "import time  # For measuring code execution time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112aabb-e303-495f-9de1-63235a8f671e",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7485769-1ff1-4f8f-8a84-f7a8382a1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = pd.read_csv('C:/Users/Nosse/IMDBDataset.csv') #load data\n",
    "print(imdb_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476f20f-31e9-4f5e-9243-bfc10b77e913",
   "metadata": {},
   "source": [
    "## 2. Review Text Pre-Processing : Cleaning Text & StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339d599-e909-449c-9da6-4cda6c66a9fe",
   "metadata": {},
   "source": [
    "In this section, we perform comprehensive text preprocessing to prepare the dataset for topic modeling and classification. As in the previous notebook, we apply several key steps: `HTML tag removal`, `special character filtering`, `tokenization`, `text normalization`, and `stemming`, which reduces words to their root forms (e.g., “running” → “run”) to unify similar terms and reduce vocabulary size.\n",
    "\n",
    "A notable enhancement in this notebook is the **removal of stopwords**, which are common, non-informative words (such as “the”, “and”, “is”) that do not contribute significantly to sentiment or topic structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0539ebaf-c542-4471-a184-15022522000e",
   "metadata": {},
   "source": [
    "### 2.1 Remove HTML tags & Noise from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d92970-027a-4b9d-9762-e8d0ef947922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html from text\n",
    "def remove_html(text):\n",
    "    \n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "        \n",
    "    ss = BeautifulSoup(text, \"html.parser\")\n",
    "    return ss.get_text()\n",
    "    \n",
    "#remove text between square brackets\n",
    "def remove_btw_square_bb(text):\n",
    "    return re.sub(r'\\[[^]]*\\]','',text)\n",
    "\n",
    "#call all the function to clean text\n",
    "def remove_noise_txt(text):\n",
    "    text = remove_html(text)\n",
    "    text = remove_btw_square_bb(text)\n",
    "    return text\n",
    "\n",
    "#apply the function\n",
    "imdb_dataset['review']= imdb_dataset['review'].apply(remove_noise_txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7172243-dfa5-4fcc-8e04-165f6f496f4a",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "1. **Removing HTML from text**: the function *remove_html(text)* uses BeautifulSoup to parse HTML content from the given text and extract only text without the tags. Useful fro cleaning reviews that may contain HTML tags.\n",
    "   \n",
    "2. **Removing text bewtween square brackets**: the function *remove_btw_square_bb(text)* utilizes a regex (regular expression), to find and remove (replace) any text that appears between square brackets, this helps to eliminate irrelevant information.\n",
    "   \n",
    "3. **Removing Noise from text**: the function  *remove_noise_txt(text)* combines the two previous functions, first remove HTML tags and then eliminates text that is enclosed in square brackets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c599462e-9d51-4193-9dd3-172e0158ccb1",
   "metadata": {},
   "source": [
    "### 2.2 Removing special CHR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1a0538-619c-4cbd-a806-e953ef394d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove special chr with regex\n",
    "def remove_sp_CHR(text, remove_digits=True):\n",
    "    pattern= r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "#Function call\n",
    "imdb_dataset['review']= imdb_dataset['review'].apply(remove_sp_CHR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f6109-2090-4614-a95f-4ab10c32ca52",
   "metadata": {},
   "source": [
    "#### Explanation: \n",
    "\n",
    "In this step, we define a function `remove_sp_CHR` that uses regular expressions (`regex`) to clean the text data. Specifically, it removes **special characters**, preserving only alphanumeric characters and whitespace. The pattern `[^a-zA-Z0-9\\s]` matches any character that is **not** a letter, digit, or space, and replaces it with an empty string. This function is applied to each review in the dataset to ensure the text is cleaner and more consistent for subsequent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87332a3f-ac5d-4479-ac13-acbd47cdfa5c",
   "metadata": {},
   "source": [
    "### 2.3 Null or Duplicate values Detection (columns - rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee49e1a-fe60-4bb8-804f-23cb1f8411be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values - each column: \n",
      " review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "\n",
      " Duplicates -  rows: \n",
      " 419\n"
     ]
    }
   ],
   "source": [
    "null_var_column = imdb_dataset.isnull().sum() #null values in dataset\n",
    "print(\"Null values - each column: \\n\",null_var_column)\n",
    "\n",
    "duplicate_rows = imdb_dataset.duplicated().sum() #duplicate rows in dataset\n",
    "print(\"\\n Duplicates -  rows: \\n\",duplicate_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2754dc1d-b8b0-4f85-969d-85c1e76d33eb",
   "metadata": {},
   "source": [
    "\n",
    "**NOTE**:\n",
    "\n",
    "In this study, **duplicate entries are intentionally retained** in the dataset to preserve a perfectly balanced distribution between sentiment classes. By maintaining the duplicates, we ensure an equal number of **25,000 positive** and **25,000 negative** reviews. This balance helps prevent bias in model training and evaluation, allowing for fairer comparisons and more reliable performance metrics during sentiment classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c5aabd-7f9c-4455-b1a5-315dc0ba9641",
   "metadata": {},
   "source": [
    "### 2.4 Text Normalization & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "711011ea-4764-4dc5-be1c-75442cc8abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nosse\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "#download  stop-words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Tokenization\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "#List of stop-words\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f30815-b242-47f4-b078-d39f533eb753",
   "metadata": {},
   "source": [
    "**NLTK Import and Stopword Download**\n",
    "The code begins by importing the NLTK (Natural Language Toolkit) library, a widely used toolkit for natural language processing. It also downloads the English stopwords set  a collection of frequently occurring words like \"and,\" \"is,\" and \"that\" which are usually excluded during preprocessing due to their limited contribution to text meaning.\n",
    "\n",
    "**Tokenizing the Text**\n",
    "A tokenizer instance using ToktokTokenizer is created. Tokenization refers to splitting the text into smaller components (typically words or phrases), facilitating easier analysis and manipulation.\n",
    "\n",
    "**Setting English Stopwords**\n",
    "Using NLTK, the code defines a list of English stopwords. This enables the model to filter out these commonly used terms that don't add significant value to sentiment interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d837c5e-e7e5-47d4-ac71-997f4504daf6",
   "metadata": {},
   "source": [
    "### 2.5 Optimization : Text Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d3c2c-a2b7-4221-b028-e02b2c78e5bc",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique used in Natural Language Processing (NLP) to reduce words to their root or base form, known as the stem. The goal is to treat words with similar meanings (such as \"running\", \"runs\", \"ran\") as a single term, thereby improving the efficiency and effectiveness of text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dab159d7-ee09-4dde-a33d-35b99041412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming Function\n",
    "def stem_txt(txt):\n",
    "    Ps = nltk.porter.PorterStemmer()\n",
    "    txt = ' '.join([Ps.stem(word) for word in txt.split()])\n",
    "    return txt\n",
    "\n",
    "#Apply f\n",
    "imdb_dataset['review']=imdb_dataset['review'].apply(stem_txt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243fc53-af61-4a6a-8f2c-7f1c93c2bef2",
   "metadata": {},
   "source": [
    "**Stem_txt** : this function applies stemming to a given text using the **Porter Stemmer** from the NLTK library.\n",
    "Splits the input text into individual words,applies stemming to each word using the **stem()** method.\n",
    "After all, rebuilds the list of stemmed words into a single string and returns the processed (stemmed) text.\n",
    "\n",
    "**The stem_txt() function is applied to every review in the review column of the imdb_dataset. The original review texts are replaced with their stemmed versions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7513ddd-3ea2-4d20-b042-8077c5e0707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    one of the other review ha mention that after ...\n",
      "1    a wonder littl product the film techniqu is ve...\n",
      "2    i thought thi wa a wonder way to spend time on...\n",
      "3    basic there a famili where a littl boy jake th...\n",
      "4    petter mattei love in the time of money is a v...\n",
      "5    probabl my alltim favorit movi a stori of self...\n",
      "6    i sure would like to see a resurrect of a up d...\n",
      "7    thi show wa an amaz fresh innov idea in the 70...\n",
      "8    encourag by the posit comment about thi film o...\n",
      "9    if you like origin gut wrench laughter you wil...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Show Stemming result on dataset\n",
    "print(imdb_dataset['review'].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
